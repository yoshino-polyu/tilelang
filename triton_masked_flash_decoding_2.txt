import torch
import triton
import triton.language as tl
from flash_attn import flash_attn_func, flash_attn_with_kvcache


def profile(func, inputs, num_warmups=100, num_iters=100):
    torch.cuda.synchronize()
    for _ in range(num_warmups):
        func(*inputs)
    torch.cuda.synchronize()
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    start.record()
    for _ in range(num_iters):
        func(*inputs)
    end.record()
    torch.cuda.synchronize()
    latency = start.elapsed_time(end) / num_iters
    return latency


def torch_masked_attention(
    q: torch.Tensor,  # [bsz, num_q_heads, qlen(=1), dim]
    k: torch.Tensor,  # [bsz*num_k_heads*max_length*dim]
    v: torch.Tensor,  # [bsz*num_k_heads*max_length*dim]
    mask: torch.Tensor,  # [bsz*num_k_heads*max_length]
    valid_length: int,
    num_k_heads: int,
):
    bsz, num_q_heads, _, dim = q.shape
    group_size = num_q_heads // num_k_heads

    q = q.view(bsz, num_k_heads, group_size, dim)
    k = k[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    v = v[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    mask = mask[:bsz*num_k_heads*valid_length].view(bsz, num_k_heads, valid_length)

    scale: float = dim ** -0.5
    attn_score = torch.einsum('bghd, bgcd -> bghc', q, k) * scale
    attn_score = attn_score.masked_fill(mask.unsqueeze(-2), -torch.inf)

    attn_lse = torch.logsumexp(attn_score, dim=-1)                 # (bsz, num_k_heads, group_size)
    attn_score = torch.nn.functional.softmax(attn_score, dim=-1)   # (bsz, num_k_heads, group_size, max_length)
    attn_out = torch.einsum('bghc, bgcd -> bghd', attn_score, v)   # (bsz, num_k_heads, group_size, dim)

    attn_lse = attn_lse.view(bsz, num_q_heads, 1)
    attn_out = attn_out.view(bsz, num_q_heads, 1, dim)
    return attn_lse, attn_out


@torch.jit.script
def torch_masked_attention_with_jit(
    q: torch.Tensor,  # [bsz, num_q_heads, qlen(=1), dim]
    k: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    v: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    mask: torch.Tensor,  # [bsz, num_k_heads, max_length]
    valid_length: int,
    num_k_heads: int,
):
    bsz, num_q_heads, _, dim = q.shape
    group_size = num_q_heads // num_k_heads

    q = q.view(bsz, num_k_heads, group_size, dim)
    k = k[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    v = v[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    mask = mask[:bsz*num_k_heads*valid_length].view(bsz, num_k_heads, valid_length)

    scale: float = dim ** -0.5
    attn_score = torch.einsum('bghd, bgcd -> bghc', q, k) * scale
    attn_score = attn_score.masked_fill(mask.unsqueeze(-2), -torch.inf)

    attn_lse = torch.logsumexp(attn_score, dim=-1)                 # (bsz, num_k_heads, group_size)
    attn_score = torch.nn.functional.softmax(attn_score, dim=-1)   # (bsz, num_k_heads, group_size, max_length)
    attn_out = torch.einsum('bghc, bgcd -> bghd', attn_score, v)   # (bsz, num_k_heads, group_size, dim)

    attn_lse = attn_lse.view(bsz, num_q_heads, 1)
    attn_out = attn_out.view(bsz, num_q_heads, 1, dim)
    return attn_lse, attn_out


def flash_attention(
    q: torch.Tensor,  # [bsz, num_q_heads, qlen(=1), dim]
    k: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    v: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    mask: torch.Tensor,  # [bsz, num_k_heads, max_length]
    valid_length: int,
    num_k_heads: int,
):
    bsz, num_q_heads, _, dim = q.shape
    k = k[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    v = v[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    attn_out, attn_lse, _ = flash_attn_func(
        q.transpose(1, 2),
        k.transpose(1, 2),
        v.transpose(1, 2),
        return_attn_probs=True,
    )
    return attn_lse, attn_out.transpose(1, 2)


def flash_decoding(
    q: torch.Tensor,  # [bsz, num_q_heads, qlen(=1), dim]
    k: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    v: torch.Tensor,  # [bsz, num_k_heads, max_length, dim]
    mask: torch.Tensor,  # [bsz, num_k_heads, max_length]
    valid_length: int,
    num_k_heads: int,
):
    bsz, num_q_heads, _, dim = q.shape
    k = k[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    v = v[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    attn_out, attn_lse = flash_attn_with_kvcache(
        q.transpose(1, 2),
        k.transpose(1, 2),
        v.transpose(1, 2),
        return_softmax_lse=True,
    )
    return attn_lse, attn_out.transpose(1, 2)


@triton.jit
def _triton_masked_attn_fwd_inner(
    q, acc, l_i, m_i,
    mask_ptrs, stride_mn,
    k_ptrs, v_ptrs, stride_kn, stride_vn,
    lo, hi, offs_n,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
    BOUNDARY_CHECK: tl.constexpr,
):
    for start_n in range(lo, hi, BLOCK_N):
        cols = start_n + offs_n
        if BOUNDARY_CHECK:
            mask = tl.load(mask_ptrs + cols * stride_mn, mask=cols < hi, other=1)
        else:
            mask = tl.load(mask_ptrs + cols * stride_mn)
        k = tl.load(k_ptrs + cols[None, :] * stride_kn)
        # -- compute qk --
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.where(mask[None, :], -1e6, qk)
        qk += tl.dot(q, k)
        # qk += tl.sum(q[:, :, None] * k[None, :, :], 1)
        # -- compute scaling constant --
        m_i_new = tl.maximum(m_i, tl.max(qk, 1))
        alpha = tl.math.exp2(m_i - m_i_new)
        p = tl.math.exp2(qk - m_i_new[:, None])
        # -- scale and update acc --
        v = tl.load(v_ptrs + cols[:, None] * stride_vn)
        acc_scale = l_i * 0 + alpha  # workaround some compiler bug
        acc *= acc_scale[:, None]
        acc += tl.dot(p.to(q.type.element_ty), v)
        # acc += tl.sum(p[:, :, None] * v[None, :, :], 1)
        # -- update m_i and l_i --
        l_i = l_i * alpha + tl.sum(p, 1)
        m_i = m_i_new
    return acc, l_i, m_i


@triton.jit
def _triton_masked_flash_decoding_kernel(
    Q, K, V,
    Lse, Out,
    Mask, valid_length,
    sm_scale,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vn, stride_vk,
    stride_oc, stride_oz, stride_oh, stride_om, stride_ok,
    stride_lc, stride_lz, stride_lh, stride_lm,
    stride_mz, stride_mh, stride_mn,
    batch_size, num_kv_heads, num_qo_heads, chunk_size,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_DMODEL: tl.constexpr,
):
    off_c = tl.program_id(0)
    off_h = tl.program_id(1)
    off_z = tl.program_id(2)

    # initialize offsets
    offs_m = tl.arange(0, BLOCK_M)
    mask_m = offs_m < num_qo_heads // num_kv_heads

    offs_n = tl.arange(0, BLOCK_N)
    offs_d = tl.arange(0, BLOCK_DMODEL)

    q_ptrs = Q + off_z * stride_qz + off_h * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk
    k_ptrs = K + off_z * stride_kz + off_h * stride_kh + offs_d[:, None] * stride_kk
    v_ptrs = V + off_z * stride_vz + off_h * stride_vh + offs_d[None, :] * stride_vk
    o_ptrs = Out + off_c * stride_oc + off_z * stride_oz + off_h * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :] * stride_ok

    lse_ptrs = Lse + off_c * stride_lc + off_z * stride_lz + off_h * stride_lh + offs_m * stride_lm
    mask_ptrs = Mask + off_z * stride_mz + off_h * stride_mh

    # initialize pointer to m and l
    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - 1e6
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)

    # scale sm_scale by log_2(e) and use
    # 2^x instead of exp in the loop because CSE and LICM
    # don't work as expected with `exp` in the loop
    qk_scale = sm_scale * 1.44269504
    # load q: it will stay in SRAM throughout
    q = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)
    q = (q * qk_scale).to(K.type.element_ty)
    # loop over k, v and update accumulator
    acc, l_i, m_i = _triton_masked_attn_fwd_inner(
        q, acc, l_i, m_i,
        mask_ptrs, stride_mn,
        k_ptrs, v_ptrs, stride_kn, stride_vn,
        off_c * chunk_size, min((off_c + 1) * chunk_size, valid_length), offs_n,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
        BOUNDARY_CHECK=True,
    )
    # log_sum_exp and output
    m_i = m_i * 0.69314718 + tl.math.log(l_i)
    acc /= l_i[:, None]

    # write back M, L
    tl.store(lse_ptrs, m_i, mask=mask_m)

    # write back O
    tl.store(o_ptrs, acc, mask=mask_m[:, None])


@triton.jit
def _triton_combine_out_and_lse_kernel(
    Out, Lse,
    stride_oc, stride_oz, stride_oh, stride_om, stride_od,
    stride_lc, stride_lz, stride_lh, stride_lm,
    num_chunks,
    BLOCK_C: tl.constexpr, BLOCK_D: tl.constexpr,
):
    off_m = tl.program_id(0)
    off_h = tl.program_id(1)
    off_z = tl.program_id(2)
    offs_c = tl.arange(0, BLOCK_C)
    offs_d = tl.arange(0, BLOCK_D)
    mask_c = offs_c < num_chunks
    o_ptr = Out + off_z * stride_oz + off_h * stride_oh + off_m * stride_om
    lse_ptr = Lse + off_z * stride_lz + off_h * stride_lh + off_m * stride_lm
    o = tl.load(o_ptr + offs_c[:, None] * stride_oc + offs_d[None, :] * stride_od, mask=mask_c[:, None], other=0.0)
    lse = tl.load(lse_ptr + offs_c * stride_lc, mask=mask_c, other=-float('inf'))
    max_lse = tl.max(lse, 0)
    exp_lse = tl.exp(lse - max_lse)
    sum_lse = tl.sum(exp_lse, 0)
    tl.store(o_ptr + offs_d * stride_od, tl.sum(o * (exp_lse / sum_lse)[:, None], 0))
    tl.store(lse_ptr, tl.log(sum_lse) + max_lse)


def triton_masked_flash_decoding(
    q: torch.Tensor,  # [bsz, num_q_heads, qlen(=1), dim]
    k: torch.Tensor,  # [bsz*num_k_heads*max_length*dim]
    v: torch.Tensor,  # [bsz*num_k_heads*max_length*dim]
    mask: torch.Tensor,  # [bsz*num_k_heads*max_length]
    valid_length: int,
    num_k_heads: int,
):
    bsz, num_q_heads, _, dim = q.shape
    group_size = num_q_heads // num_k_heads
    sm_scale = dim ** -0.5

    chunk_size = min(triton.next_power_of_2(bsz * 128), valid_length)
    num_chunks = triton.cdiv(valid_length, chunk_size)

    block_M = 16
    block_N = 64

    q = q.view(bsz, num_k_heads, group_size, dim)
    k = k[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    v = v[:bsz*num_k_heads*valid_length*dim].view(bsz, num_k_heads, valid_length, dim)
    mask = mask[:bsz*num_k_heads*valid_length].view(bsz, num_k_heads, valid_length)

    o = torch.empty((num_chunks, bsz, num_k_heads, group_size, dim), dtype=torch.float32, device=q.device)
    lse = torch.empty((num_chunks, bsz, num_k_heads, group_size), dtype=torch.float32, device=q.device)

    _triton_masked_flash_decoding_kernel[(num_chunks, num_k_heads, bsz)](
        q, k, v,
        lse, o,
        mask, valid_length,
        sm_scale,
        q.stride(0), q.stride(1), q.stride(2), q.stride(3),
        k.stride(0), k.stride(1), k.stride(2), k.stride(3),
        v.stride(0), v.stride(1), v.stride(2), v.stride(3),
        o.stride(0), o.stride(1), o.stride(2), o.stride(3), o.stride(4),
        lse.stride(0), lse.stride(1), lse.stride(2), lse.stride(3),
        mask.stride(0), mask.stride(1), mask.stride(2),
        bsz, num_k_heads, num_q_heads, chunk_size,
        BLOCK_M=block_M, BLOCK_N=block_N,
        BLOCK_DMODEL=dim,
        num_warps=4, num_stages=4,
    )

    if num_chunks > 1:
        # lse_softmax = torch.nn.functional.softmax(lse, dim=0)
        # o = ((o * lse_softmax.unsqueeze(-1)).sum(0))
        # lse = (lse[0] - torch.log(lse_softmax[0]))  # = torch.logsumexp(lse, dim=0)
        block_C = triton.next_power_of_2(num_chunks)
        _triton_combine_out_and_lse_kernel[(group_size, num_k_heads, bsz)](
            o, lse,
            o.stride(0), o.stride(1), o.stride(2), o.stride(3), o.stride(4),
            lse.stride(0), lse.stride(1), lse.stride(2), lse.stride(3),
            num_chunks, BLOCK_C=block_C, BLOCK_D=dim,
            num_warps=4, num_stages=1,
        )
    o = o[0]
    lse = lse[0]

    return lse.reshape(bsz, num_q_heads, 1), o.to(q.dtype).reshape(bsz, num_q_heads, 1, dim)


def profile_all(funcs, inputs):
    results = {}
    max_tag_length = max(map(len, funcs.keys()))
    for tag, func in funcs.items():
        latency = profile(func, inputs)
        results[tag] = latency
        # print(f'{tag:{max_tag_length}} : {latency:5.3f} ms')
    return results


def test_masked_decoding(
    bsz: int,
    num_k_heads: int,
    num_q_heads: int,
    max_length: int,
    dim: int,
    device: torch.device = 'cuda',
    dtype: torch.dtype = torch.float16,
):
    # print(f'bsz={bsz}, num_k_heads={num_k_heads}, num_q_heads={num_q_heads}, max_length={max_length}, dim={dim}')
    q = torch.randn((bsz, num_q_heads, 1, dim), dtype=dtype, device=device)
    k = torch.randn((bsz*num_k_heads*max_length*dim, ), dtype=dtype, device=device)
    v = torch.randn((bsz*num_k_heads*max_length*dim, ), dtype=dtype, device=device)
    mask = torch.randint(0, 2, (bsz*num_k_heads*max_length, ), dtype=torch.bool, device=device)
    valid_length = max_length // 2
    inputs = [q, k, v, mask, valid_length, num_k_heads]

    ref_lse, ref_out = torch_masked_attention(*inputs)
    lse, out = triton_masked_flash_decoding(*inputs)
    torch.testing.assert_close(out, ref_out, atol=1e-3, rtol=1e-3)
    torch.testing.assert_close(lse, ref_lse.to(lse.dtype), atol=1e-3, rtol=1e-3)

    return profile_all({
        'flash_attn': flash_attention,
        'flash_decoding': flash_decoding,
        'torch_masked_attn': torch_masked_attention,
        'torch_masked_attn_jit': torch_masked_attention_with_jit,
        'triton_masked_flash_decoding': triton_masked_flash_decoding,
    }, inputs)


if __name__ == '__main__':
    from tqdm import tqdm
    import pandas as pd
    df = []
    for bsz in tqdm([1, 2, 4, 8]):
        results = test_masked_decoding(
            bsz=bsz,
            num_k_heads=8,
            num_q_heads=32,
            max_length=131072,
            dim=128,
        )
        df.append(pd.DataFrame({bsz: results}).T)
    df = pd.concat(df).round(3)
    df.index.name = 'bsz'
    print(df)
